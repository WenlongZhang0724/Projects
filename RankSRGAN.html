<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>RankSRGAN: Generative Adversarial Networks with Ranker for Image Super-Resolution</title>
  <!--=================Meta tags==========================-->
  <meta name="robots" content="index,follow">
  <meta name="description"
    content="Video restoration tasks, including super-resolution, deblurring, etc, are drawing increasing attention in the computer vision community. A challenging benchmark named REDS is released in the NTIRE19 Challenge. This new benchmark challenges existing methods from two aspects: (1) how to align multiple frames given large motions, and (2) how to effectively fuse different frames with diverse motion and blur. In this work, we propose a novel Video Restoration framework with Enhanced Deformable networks, termed EDVR, to address these challenges. First, to handle large motions, we devise a Pyramid, Cascading and Deformable (PCD) alignment module, in which frame alignment is done at the feature level using deformable convolutions in a coarse-to-fine manner. Second, we propose a Temporal and Spatial Attention (TSA) fusion module, in which attention is applied both temporally and spatially, so as to emphasize important features for subsequent restoration. Thanks to these modules, our EDVR wins the champions and outperforms the second place by a large margin in all four tracks in the NTIRE19 video restoration and enhancement challenges. EDVR also demonstrates superior performance to state-of-the-art published methods on video super-resolution and deblurring. The code is available at https://github.com/xinntao/EDVR.">
  <meta name="keywords"
    content="video, super resolution, deblurring, denoise, video restoration, deformable, pyramid, attention, EDVR">
  <link rel="author" href="https://wenlongzhang0724.github.io/RankSRGAN">
  <!--=================js==========================-->
  <link href="./css.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" type="text/css" href="./project.css" media="screen">
  <script src="./effect.js "></script>
  <!-- Latex -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
      </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
  <!--=================Google Analytics==========================-->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-129775907-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-129775907-1');
  </script>
</head>

<body>
  <div id="content">
    <div id="content-inner">
      <div class="section head">
        <h1>
          <font color="Tomato">RankSRGAN</font>: <font color="Tomato">G</font>enerative <font color="Tomato">A</font>dversarial
          <font color="Tomato">N</font>etworks with <font color="Tomato">Rank</font>er for Image <font color="Tomato">S</font>uper-
          <font color="Tomato">R</font>esolution
        </h1>
        <!--=================Authors==========================-->
        <div class="authors">
          <a href="https://wenlongzhang0517.github.io/" target="_blank">Wenlong Zhang</a>  &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="http://xpixel.group/2010/03/29/yihaoliu.html" target="_blank">Yihao Liu</a>  &nbsp;&nbsp;&nbsp;&nbsp;
          <a href=" https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ" target="_blank">Chao Dong</a>
           &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="http://mmlab.siat.ac.cn/yuqiao/index.html" target="_blank">Yu Qiao</a> 
        </div>

        <div class="affiliations ">
          SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences
        </div>
        <!--=================Tabs==========================-->
        <ul id="tabs">
          <li><a href="#materials" name="#tab1">Materials</a></li>
          <li><a href="#RankSRGAN" name="#tab2">RankSRGAN</a></li>
          <li><a href="#ablations" name="#tab3">Ablations</a></li>
          <li><a href="#results" name="#tab4">Results</a></li>
          <li><a href="#citation" name="#tab5">Citation</a></li>
      </div>
      <br>
      <!--=================Teasers==========================-->

<!--       <div id="img_intro_examples" class="img_container">
        <center>
          <img style="width:90%" src="./RankSRGAN_src/visual_results1.png">
        </center>
      </div> -->

      <div id="img_intro_examples" class="img_container">
        <center>
          <div class="leftView">
            <div class="mask" style="width:65px;height:80px"></div>
            <img class='small' src="./RankSRGAN_src/visual_results1.png">
          </div>
        </center>
      </div>
      <div class="section">
<!--         <p><b>The EDVR framework</b>. It is a unified framework suitable for various video restoration tasks,
          <i>e.g.</i>, super-resolution and
          deblurring. Inputs with high spatial resolution are first down-sampled to reduce computational cost. Given
          blurry inputs, a PreDeblur
          Module is inserted before the PCD Align Module to improve alignment accuracy. We use three input frames as an
          illustrative example.</p> -->
      </div>

      <!--=================Abstract==========================-->
      <div class="section abstract">
        <h2>Abstract</h2>
        <br>
        <p>
            Generative Adversarial Networks (GAN) have demonstrated the potential to recover realistic details for single
            image super-resolution (SISR). To further improve the visual quality of super-resolved results, <b>PIRM2018-SR 
            Challenge</b> employed perceptual metrics to assess the perceptual quality, such as <b>PI, NIQE, and Ma</b>. However, 
            existing methods cannot directly optimize these <b>indifferentiable</b> perceptual metrics, which are shown to be highly correlated with
            human ratings. To address the problem, we propose Super-Resolution Generative Adversarial Networks with Ranker (<b>RankSRGAN</b>) to optimize generator in the direction of perceptual metrics. Specifically, we first train a <b>Ranker</b> which can learn the behavior of perceptual metrics and then introduce a novel <b>rank-content loss</b> to optimize the perceptual quality.
            The most appealing part is that the proposed method can
            combine the strengths of different SR methods to generate
            better results. Extensive experiments show that RankSRGAN achieves visually pleasing results and reaches stateof-the-art 
            performance in perceptual metrics.
        </p>
      </div>

      <!--=================Highlights==========================-->
      <div class="section abstract">
        <h2>Contributions</h2>
        <ol>
          <li>We propose a general perceptual SR framework â€“ <b>RankSRGAN</b> that could optimize generator in the <b>direction of
            indifferentiable perceptual metrics</b> and achieve the <b>state-of-the-art</b> performance.</li>
          <li>We, for the first time, utilize results of other SR methods to build the training dataset.
              The proposed RankSRGAN could <b>combine the strengths of different SR methods</b> and generate better results.
          </li>
          <li>The proposed SR framework is highly <b>flexible</b> and could produce diverse results given different rank datasets, perceptual
              metrics, and loss combinations.
          </li>

        </ol>
      </div>
      <!--=================Materials==========================-->
      <div class="section materials" , id="materials">
        <h2>Materials</h2>
        <table width="100%" align="center" border=none cellspacing="0" cellpadding="30">
          <tr>
            <td width="80%">
              <center>
                <a href="https://arxiv.org/abs/1908.06382" target="_blank" class="imageLink"><img src="./RankSRGAN_src/suoluetu.png" ,
                    width="80%"></a><br><br>
                <a href="https://arxiv.org/abs/1908.06382" target="_blank">Paper</a>
              </center>
            </td>

            <td width="20%" valign="middle">
              <center>
                <a href="https://github.com/WenlongZhang0724/RankSRGAN" target="_blank" class="imageLink"><img src="./icon_github.png"
                    , width="50%"></a><br><br>
                <a href="https://github.com/WenlongZhang0724/RankSRGAN" target="_blank">Codes</a>
              </center>
            </td>
          </tr>
        </table>
      </div>


      <!--=================RankSRGAN Modules==========================-->
      <div class="section" , id="RankSRGAN">
        <h2> RankSRGAN Framework</h2>
        <p>
          <b>The overview of the proposed RankSRGAN method:</b>
        </p>
        <div id="sr" class="img_container">
          <center>
            <img style="width:80%" src="./RankSRGAN_src/method.png">
          </center>
        </div>
        <p>
          <b>Stage 1:</b> Generate pair-wise rank images by different SR models in the orientation of
          perceptual metrics. <b>Stage 2:</b> Train Siamese-like Ranker network. <b>Stage 3:</b> Introduce rank-content loss derived from 
          well-trained Ranker to guide GAN training. RankSRGAN consists of a generator(G), discriminator(D), a fixed Feature extractor(F) and Ranker(R)
        </p>
        
        <p>
          We show the convergence curves of RankSRGAN. RankSRGAN is
          consistently better than SRGAN by a large margin on NIQE.
        </p>
        <div id="sr" class="img_container">
          <center>
            <img style="width:90%" src="./RankSRGAN_src/curve.png">
          </center>
        </div>
        
      </div>
      <br>

      <!--=================Ablation studies==========================-->
      <div class="section" , id="ablations">
        <h2>Ablation Studies on RankSRGAN</h2>
        <h3>1. Effect of different rank datasets</h3>
        <p>
          We show the convergence curves of RankSRGAN with different rank datasets in NIQE and PSNR. 
          RankSRGAN uses the Ranker with rank dataset <b>(SRResNet, SRGAN, ESRGAN)</b>. 
          RankSRGAN-HR employs the Ranker with rank dataset <b>(SRResNet, SRGAN, HR)</b>. 
        </p>
        <div id="as1" class="img_container">
          <center>
            <img style="width:90%" src="./RankSRGAN_src/ranksrgan_hr.png">
          </center>
        </div>
      <br>

      <!--=================2==========================-->
        <h3>2. Effect of Ranker: Rank VS. Regression</h3>

        <p>
          We show the hist distribution of Ranker in NIQE and Ma metrics. The convergence curves are shown in section 3.
        </p>
        <div id="as2" class="img_container">
          <center>
            <img style="width:90%" src="./RankSRGAN_src/hist.png">
          </center>
        </div>
        <br>

      <!--=================3==========================-->
        <h3>3. Effect of different perceptual metrics</h3>
        <p>
          We present the convergence curves showing that our RankSRGAN can achieve a constant improvement compared with
          the baseline SRGAN on different perceptual metrics <b>(NIQE, Ma and PI)</b>. 
        </p>
        <div id="as3" class="img_container">
          <center>
            <img style="width:90%" src="./RankSRGAN_src/SF.png">
          </center>
        </div>
      <br>

      <!--=================4==========================-->
        <h3>4. Effect of Ranker with different architectures</h3>
        <p>
          We train three VGG networks varying from shallow to
          deep ones: VGG-8, VGG-12 and VGG-16.  Since the VGG-12 can achieve the same accuracy as VGG-16 (SROCC 0.88), 
          we apply the VGG-8 (SROCC 0.83) and VGG-12 (SROCC 0.88) on RankSRGAN.
          The figure shows the performance of RankSRGAN with different Rankers. The Ranker with higher value of
          SROCC can achieve better performance when applied on RankSRGAN. 
        </p>
        <div id="as4" class="img_container">
          <center>
            <img style="width:90%" src="./RankSRGAN_src/ranksrgan_r.png">
          </center>
        </div>
      <br>
        <h3>5. Effect of Rank dataset with different sizes</h3>
        <p>
          We employ DIV2K to generate rank dataset1 (D1) with <b>15</b> K image pairs and
          use DIV2K+Flickr2k to generate rank dataset2 (D2) with <b>150 K</b> image pairs. Then, we utilize rank dataset1 and rank dataset2
          to train Ranker1 (SROCC 0.78) and Ranker2 (SROCC 0.88), respectively. The convergence curves show that the Ranker2 with higher SROCC can reach better performance in
          NIQE and PSNR.
        </p>
        <div id="as5" class="img_container">
          <center>
            <img style="width:90%" src="./RankSRGAN_src/RankSRGAN_data.png">
          </center>
        </div>
      </div>
      <br>
      <!--=================Applications==========================-->
      <div class="section" , id="results">
        <h2>Results</h2>
        <!--=================*******==========================-->
        <h3>Visual results</h3>

        <div id="vid1" class="img_container">
          <center>
            <img style="width:90%" src="./RankSRGAN_src/VS__3.png">
          </center>
        </div>

        <div id="vid2" class="img_container">
          <center>
            <img style="width:90%" src="./RankSRGAN_src/VS__2.png">
          </center>
        </div>

        <div id="vid3" class="img_container">
          <center>
            <img style="width:90%" src="./RankSRGAN_src/VS__1.png">
          </center>
        </div>
        <br>





        <!--=================*******==========================-->
<!--         <h3>2. Video Super-Resolution on Vimeo90K Dataset</h3>
        <div id="vimeo90k" class="img_container">
          <center>
            <div class="leftView">
              <div class="mask" style="width:70px;height:70px"></div>
              <img class='small' src="./EDVR_src/vimeo90k.jpg">
            </div>
          </center>
        </div>
        <br> -->
        <!--=================*******==========================-->
<!--         <h3>3. Video Deblurring on REDS4 Dataset</h3>
        <div id="reds4" class="img_container">
          <center>
            <div class="leftView">
              <div class="mask" style="width:70px;height:70px"></div>
              <img class='small' src="./EDVR_src/reds_deblur.jpg">
            </div>
          </center>
        </div>
        <br> -->
        <!--=================*******==========================-->
<!--         <h3>4. Competition Results on REDS4 Dataset</h3>
        <div id="reds4" class="img_container">
          <center>
            <div class="leftView">
              <div class="mask" style="width:70px;height:70px"></div>
              <img class='small' src="./EDVR_src/reds.jpg">
            </div>
          </center>
        </div>
        <br>
      </div> -->
      <!--=================Citation==========================-->
<!--       <div class="section citation" , id="citation">
        <h2>Citation</h2>
        <div class="section bibtex">
          <pre>@InProceedings{wang2019edvr,
          author = {Wang, Xintao and Chan, Kelvin C.K. and Yu, Ke and Dong, Chao and Loy, Chen Change},
          title = {EDVR: Video Restoration with Enhanced Deformable Convolutional Networks},
          booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
          month = {June},
          year = {2019}
          }
          </pre>
          <pre>@Article{tian2018tdan,
          author = {Tian, Yapeng and Zhang, Yulun and Fu, Yun and Xu, Chenliang},
          title = {TDAN: Temporally deformable alignment network for video super-resolution},
          journal = {arXiv preprint arXiv:1812.02898},
          year = {2018}
          }
          </pre>
        </div>
      </div> -->
      <!--=================Contact==========================-->
<!--       <div class="section contact">

      </div> -->

      <!--=================Contact==========================-->
      <div class="section contact">
        <h2 id="contact">Acknowledgement</h2>
        <p>Thanks to <a href="https://xinntao.github.io/" target="_blank">Xintao Wang</a>, who discussed with us the details of ESRGAN/RankSRGAN , code architecture and user study, etc. </p> 

        <h2 id="contact">Contact</h2>
        <p>If you have any question, please contact Wenlong Zhang at <strong>zhangwenlong0724@gmail.com</strong>.</p>


      </div>
</body>

</html>
